*** Begin Patch
*** Update File: main.py
@@
 def handler14_kpmg_insights(chrome_page_render: ChromePageRender, document: HTMLDocument, url_name: str, url_info: dict) -> None:
@@
     return None
+
+
+def handler15_mck_insights(chrome_page_render: ChromePageRender, document: HTMLDocument, url_name: str, url_info: dict) -> None:
+    """
+    麦肯锡中国（McKinsey & Company）（洞察）列表页处理器
+
+    行为概述：
+    - 打开 insights 列表页并等待主体容器出现；
+    - 若出现 Cookie/隐私弹窗，尝试接受；
+    - 通过连续下滑触发列表懒加载，直到收集到 MaxItems 条或页面不再增长；
+    - 解析条目 (link, title, date) 并以统一结构输出。
+
+    选择器策略（尽量健壮）：
+    - 根容器：'main' 或包含 insights 列表的 section/article 容器
+    - 条目锚点：优先匹配含 '/insights/' 的链接；排除导航/页脚
+    - 标题：h3/h2/含 title 类名的元素；兜底 a 的文本
+    - 日期：<time datetime> 或带有 date 类名的文本，规范为 YYYY-MM-DD
+    """
+    urls = url_info.get('URLs', []) or []
+    if not urls:
+        return None
+
+    base_url = urls[0]
+    max_items = int(url_info.get('MaxItems', 20))
+
+    def _try_accept_cookies():
+        candidates = [
+            ('css', 'button#onetrust-accept-btn-handler'),
+            ('css', 'button[aria-label*="accept" i]'),
+            ('css', 'button[title*="接受" i], button[title*="同意" i]'),
+            ('xpath', "//button[contains(., '接受') or contains(., '同意') or contains(., '允许')]"),
+        ]
+        for (typ, sel) in candidates:
+            try:
+                is_timeout = chrome_page_render.click_on_html_element(
+                    click_element_selector_type=typ,
+                    click_element_selector_rule=sel,
+                    use_javascript=True,
+                    max_trials_for_unstable_page=2,
+                    click_waiting_timeout_in_seconds=min(5, url_info.get('WaitingTimeLimitInSeconds', 10)),
+                    print_error_log_to_console=False
+                )
+                if is_timeout is False:
+                    break
+            except Exception:
+                continue
+
+    def _parse_date_to_iso(s: str) -> str:
+        try:
+            if not isinstance(s, str):
+                return ''
+            s = s.strip()
+            if not s:
+                return ''
+            m = re.search(r'(\d{4})[./-](\d{1,2})[./-](\d{1,2})', s)
+            if m:
+                return f"{int(m.group(1)):04d}-{int(m.group(2)):02d}-{int(m.group(3)):02d}"
+            m = re.search(r'(\d{4})\s*年\s*(\d{1,2})\s*月\s*(\d{1,2})\s*日', s)
+            if m:
+                return f"{int(m.group(1)):04d}-{int(m.group(2)):02d}-{int(m.group(3)):02d}"
+        except Exception:
+            pass
+        return ''
+
+    def _extract_items(html: str, base: str):
+        soup = BeautifulSoup(html, 'html.parser')
+        # 根作用域：优先 main，其次默认 soup
+        root = soup.select_one('main') or soup
+        items = []
+        seen = set()
+        # 优先网格/列表容器
+        containers = [
+            root.select_one('section[class*="insight" i]'),
+            root.select_one('div[class*="insight" i]'),
+            root.select_one('section[class*="list" i]'),
+            root
+        ]
+        for container in containers:
+            if not container:
+                continue
+            # 候选卡片节点：article/li/div 块
+            cards = container.select('article, li, div')
+            for node in cards:
+                # 排除导航/页脚
+                if node.find_parent('nav') is not None or 'footer' in (getattr(node, 'name', '') or '').lower():
+                    continue
+                a = node.select_one('a[href*="/insights/" i]') or node.select_one('a[href]')
+                if not a or not a.get('href'):
+                    continue
+                href = url_join(base, a['href'])
+                if href in seen:
+                    continue
+                title_node = (
+                    node.select_one('h3, h2') or a.select_one('h3, h2') or node.select_one('[class*="title" i]') or a
+                )
+                title = title_node.get_text(strip=True) if title_node is not None else ''
+                if not title:
+                    continue
+                date_node = node.select_one('time[datetime]') or node.select_one('time') or node.select_one('[class*="date" i]')
+                date_text = ''
+                if date_node is not None:
+                    date_text = _parse_date_to_iso(date_node.get('datetime', '') or date_node.get_text(strip=True))
+                items.append((href, title, date_text))
+                seen.add(href)
+                if len(items) >= max_items:
+                    break
+            if len(items) >= max_items:
+                break
+        return items
+
+    # 打开列表页
+    try:
+        chrome_page_render.goto_url(url=base_url)
+    except Exception:
+        pass
+
+    _try_accept_cookies()
+    # 等待 main/列表容器出现（软等待）
+    try:
+        chrome_page_render.wait_for_selectors(
+            wait_type='appear',
+            selector_types_rules=url_info.get('RulesAwaitingSelectors(Types,Rules)', [('css', 'main')]),
+            waiting_timeout_in_seconds=url_info.get('WaitingTimeLimitInSeconds', 10),
+            print_error_log_to_console=False
+        )
+    except Exception:
+        pass
+
+    results = []
+    last_len = 0
+    stagnation_rounds = 0
+
+    def _exec_js(script: str):
+        try:
+            browser = getattr(chrome_page_render, f"_ChromePageRender__browser", None)
+            if browser is not None:
+                return browser.execute_script(script)
+        except Exception:
+            pass
+        return None
+
+    # 初次解析
+    max_wait = max(6, int(url_info.get('WaitingTimeLimitInSeconds', 10)) * 2)
+    for _ in range(max_wait):
+        try:
+            html = chrome_page_render.get_page_source()
+        except Exception:
+            html = ''
+        results = _extract_items(html, base_url)
+        if results:
+            break
+        sleep(0.5)
+
+    # 连续下滑以触发懒加载（无 Load More 按钮）
+    while len(results) < max_items:
+        try:
+            before_h = _exec_js('return Math.max(document.body.scrollHeight, document.documentElement.scrollHeight);') or 0
+            _exec_js('window.scrollTo(0, document.body.scrollHeight);')
+            sleep(0.9)
+            after_h = _exec_js('return Math.max(document.body.scrollHeight, document.documentElement.scrollHeight);') or before_h
+        except Exception:
+            after_h = 0
+
+        _try_accept_cookies()
+
+        try:
+            html = chrome_page_render.get_page_source()
+        except Exception:
+            html = ''
+        current = _extract_items(html, base_url)
+        if current:
+            seen = set(x[0] for x in results)
+            for it in current:
+                if it[0] not in seen:
+                    results.append(it)
+                    seen.add(it[0])
+
+        if len(results) == last_len and (after_h <= (before_h or 0)):
+            stagnation_rounds += 1
+        else:
+            stagnation_rounds = 0
+        last_len = len(results)
+        if stagnation_rounds >= 3:
+            break
+
+    # 渲染到聚合页
+    with document.body:
+        with HTMLTags.div(cls='page-board'):
+            HTMLTags.img(cls='site-logo', src=url_info['LogoPath'], alt='Missing Logo')
+            with HTMLTags.a(href=base_url):
+                HTMLTags.h2(url_name)
+            for (a_href, h3_text, span_text) in results[:max_items]:
+                with HTMLTags.div(cls='page-board-item'):
+                    with HTMLTags.a(href=a_href):
+                        HTMLTags.h3(h3_text)
+                        HTMLTags.span(span_text or '')
+    return None
@@
 URLData.update(KPMG_URLData)
+
+# 麦肯锡中国（洞察）
+MCK_URLData = {
+    '麦肯锡中国（McKinsey & Company）（洞察）': {
+        'URLs': [
+            'https://www.mckinsey.com.cn/insights/',
+        ],
+        'RulesAwaitingSelectors(Types,Rules)': [
+            ('css', 'main'),
+        ],
+        'WaitingTimeLimitInSeconds': 30,
+        'LogoPath': './Logos/handler15_McK_zh.png',
+        'MaxItems': 20,
+        'HTMLContentHandler': handler15_mck_insights,
+    },
+}
+
+URLData.update(MCK_URLData)
*** End Patch
